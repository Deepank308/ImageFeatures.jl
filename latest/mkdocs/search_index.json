{
    "docs": [
        {
            "location": "/", 
            "text": "ImageFeatures.jl\n\n\n\n\nIntroduction\n\n\nThe ideal keypoint detector finds salient image regions such that they are repeatably detected despite change of viewpoint and more generally it is robust to all possible image transformations. Similarly, the ideal keypoint descriptor captures the most important and distinctive information content enclosed in the detected salient regions, such that the same structure can be recognized if encountered. \n\n\n\n\nInstallation\n\n\nInstalling the package is extremely easy with julia's package manager - \n\n\nPkg.clone(\nhttps://github.com/JuliaImages/ImageFeatures.jl\n)\n\n\n\n\nImageFeatures.jl requires \nImages.jl\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#imagefeaturesjl", 
            "text": "", 
            "title": "ImageFeatures.jl"
        }, 
        {
            "location": "/#introduction", 
            "text": "The ideal keypoint detector finds salient image regions such that they are repeatably detected despite change of viewpoint and more generally it is robust to all possible image transformations. Similarly, the ideal keypoint descriptor captures the most important and distinctive information content enclosed in the detected salient regions, such that the same structure can be recognized if encountered.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#installation", 
            "text": "Installing the package is extremely easy with julia's package manager -   Pkg.clone( https://github.com/JuliaImages/ImageFeatures.jl )  ImageFeatures.jl requires  Images.jl .", 
            "title": "Installation"
        }, 
        {
            "location": "/tutorials/feature_matching/", 
            "text": "", 
            "title": "Feature Matching"
        }, 
        {
            "location": "/tutorials/brief/", 
            "text": "BRIEF\n (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests. BRIEF does not have a sampling pattern thus pairs can be chosen at any point on the \nSxS\n patch.\n\n\nTo build a BRIEF descriptor of length \nn\n, we need to determine \nn\n pairs \n(Xi,Yi)\n. Denote by \nX\n and \nY\n the vectors of point \nXi\n and \nYi\n, respectively.\n\n\nIn ImageFeatures.jl we have five methods to determine the vectors \nX\n and \nY\n :\n\n\n\n\nrandom_uniform\n : \nX\n and \nY\n are randomly uniformly sampled\n\n\ngaussian\n : \nX\n and \nY\n are randomly sampled using a Gaussian distribution, meaning that locations that are closer to the center of the patch are preferred\n\n\ngaussian_local\n : \nX\n and \nY\n are randomly sampled using a Gaussian distribution where first \nX\n is sampled with a standard deviation of \n0.04*S^2\n and then the \nYi\u2019s\n are sampled using a Gaussian distribution \u2013 Each \nYi\n is sampled with mean \nXi\n and standard deviation of \n0.01 * S^2\n\n\nrandom_coarse\n : \nX\n and \nY\n are randomly sampled from discrete location of a coarse polar grid\n\n\ncentered\n : For each \ni\n, \nXi\n is \n(0, 0)\n and \nYi\n takes all possible values on a coarse polar grid\n\n\n\n\nAs with all the binary descriptors, BRIEF\u2019s distance measure is the number of different bits between two binary strings which can also be computed as the sum of the XOR operation between the strings.\n\n\nBRIEF is a very simple feature descriptor and does not provide scale or rotation invariance (only translation invariance). To achieve those, see \nORB\n, \nBRISK\n and \nFREAK\n.\n\n\n\n\nExample\n\n\nLet us take a look at a simple example where the BRIEF descriptor is used to match two images where one has been translated by \n(10, 20)\n pixels. We will use the \nlena_gray\n image from the \nTestImages\n package for this example.\n\n\nFirst, let us define a warping function to transform the image.\n\n\nfunction _warp(img, transx, transy)\n    res = zeros(eltype(img), size(img))\n    for i in 1:size(img, 1) - transx\n        for j in 1:size(img, 2) - transy\n            res[i + transx, j + transy] = img[i, j]\n        end\n    end\n    res = shareproperties(img, res)\n    res\nend\n\n\n\n\nNow, let us create the two images we will match using BRIEF.\n\n\nusing ImageFeatures, TestImages, Images, ImageDraw\n\nimg = testimage(\nlena_gray_512\n)\nimg_array_1 = convert(Array{Images.Gray}, img)\nimg_array_2 = _warp(img_array_1, 10, 20)\n\n\n\n\nTo calculate the descriptors, we first need to get the keypoints. For this tutorial, we will use the FAST corners to generate keypoints (see \nfastcorners\n.\n\n\nkeypoints_1 = Keypoints(fastcorners(img_array_1, 12, 0.4))\nkeypoints_2 = Keypoints(fastcorners(img_array_2, 12, 0.4))\n\n\n\n\nTo create the BRIEF descriptor, we first need to define the parameters by calling the \nBRIEF\n constructor.\n\n\nbrief_params = BRIEF(size = 256, window = 10, seed = 123)\n\n\n\n\nNow pass the image with the keypoints and the parameters to the \ncreate_descriptor\n function.\n\n\ndesc_1, ret_keypoints_1 = create_descriptor(img_array_1, keypoints_1, brief_params)\ndesc_2, ret_keypoints_2 = create_descriptor(img_array_2, keypoints_2, brief_params)\n\n\n\n\nThe obtained descriptors can be used to find the matches between the two images using the \nmatch_keypoints\n function.\n\n\nmatches = match_keypoints(ret_keypoints_1, ret_keypoints_2, desc_1, desc_2, 0.1)\n\n\n\n\nWe can use the \nImageDraw.jl\n package to view the results.\n\n\ngrid = hcat(img_array_1, img_array_2)\noffset = CartesianIndex(0, 512)\nmap(m_i -\n line!(grid, m_i[1], m_i[2] + offset), matches)", 
            "title": "BRIEF"
        }, 
        {
            "location": "/tutorials/brief/#example", 
            "text": "Let us take a look at a simple example where the BRIEF descriptor is used to match two images where one has been translated by  (10, 20)  pixels. We will use the  lena_gray  image from the  TestImages  package for this example.  First, let us define a warping function to transform the image.  function _warp(img, transx, transy)\n    res = zeros(eltype(img), size(img))\n    for i in 1:size(img, 1) - transx\n        for j in 1:size(img, 2) - transy\n            res[i + transx, j + transy] = img[i, j]\n        end\n    end\n    res = shareproperties(img, res)\n    res\nend  Now, let us create the two images we will match using BRIEF.  using ImageFeatures, TestImages, Images, ImageDraw\n\nimg = testimage( lena_gray_512 )\nimg_array_1 = convert(Array{Images.Gray}, img)\nimg_array_2 = _warp(img_array_1, 10, 20)  To calculate the descriptors, we first need to get the keypoints. For this tutorial, we will use the FAST corners to generate keypoints (see  fastcorners .  keypoints_1 = Keypoints(fastcorners(img_array_1, 12, 0.4))\nkeypoints_2 = Keypoints(fastcorners(img_array_2, 12, 0.4))  To create the BRIEF descriptor, we first need to define the parameters by calling the  BRIEF  constructor.  brief_params = BRIEF(size = 256, window = 10, seed = 123)  Now pass the image with the keypoints and the parameters to the  create_descriptor  function.  desc_1, ret_keypoints_1 = create_descriptor(img_array_1, keypoints_1, brief_params)\ndesc_2, ret_keypoints_2 = create_descriptor(img_array_2, keypoints_2, brief_params)  The obtained descriptors can be used to find the matches between the two images using the  match_keypoints  function.  matches = match_keypoints(ret_keypoints_1, ret_keypoints_2, desc_1, desc_2, 0.1)  We can use the  ImageDraw.jl  package to view the results.  grid = hcat(img_array_1, img_array_2)\noffset = CartesianIndex(0, 512)\nmap(m_i -  line!(grid, m_i[1], m_i[2] + offset), matches)", 
            "title": "Example"
        }, 
        {
            "location": "/tutorials/orb/", 
            "text": "The \nORB\n descriptor is a somewhat similar to \nBRIEF\n. It doesn\u2019t have an elaborate sampling pattern as \nBRISK\n or \nFREAK\n. \n\n\nHowever, there are two main differences between ORB and BRIEF: - ORB uses an orientation compensation mechanism, making it rotation invariant. - ORB learns the optimal sampling pairs, whereas BRIEF uses randomly chosen sampling pairs.\n\n\nThe ORB descriptor uses the intensity centroid as a measure of orientation. To calculate the centroid, we first need to find the moment of a patch, which is given by \nMpq = x,yxpyqI(x,y)\n. The centroid, or \u2018centre of mass' is then given by \nC=(M10M00, M01M00)\n.\n\n\nThe vector from the corner\u2019s center to the centroid gives the orientation of the patch. Now, the patch can be rotated to some predefined canonical orientation before calculating the descriptor, thus achieving rotation invariance.\n\n\nORB tries to take sampling pairs which are uncorrelated so that each new pair will bring new information to the descriptor, thus maximizing the amount of information the descriptor carries. We also want high variance among the pairs making a feature more discriminative, since it responds differently to inputs. To do this, we consider the sampling pairs over keypoints in standard datasets and then do a greedy evaluation of all the pairs in order of distance from mean till the number of desired pairs are obtained i.e. the size of the descriptor.\n\n\nThe descriptor is built using intensity comparisons of the pairs. For each pair if the first point has greater intensity than the second, then 1 is written else 0 is written to the corresponding bit of the descriptor.\n\n\n\n\nExample\n\n\nLet us take a look at a simple example where the ORB descriptor is used to match two images where one has been translated by \n(50, 40)\n pixels and then rotated by an angle of 75 degrees. We will use the \nlighthouse\n image from the \nTestImages\n package for this example.\n\n\nFirst, lets define warping functions to transform and rotate the image.\n\n\nfunction _warp(img, transx, transy)\n    res = zeros(eltype(img), size(img))\n    for i in 1:size(img, 1) - transx\n        for j in 1:size(img, 2) - transy\n            res[i + transx, j + transy] = img[i, j]\n        end\n    end\n    res = shareproperties(img, res)\n    res\nend\n\nfunction _warp(img, angle)\n    cos_angle = cos(angle)\n    sin_angle = sin(angle)\n    res = zeros(eltype(img), size(img))\n    cx = size(img, 1) / 2\n    cy = size(img, 2) / 2\n    for i in 1:size(res, 1)\n        for j in 1:size(res, 2)\n            i_rot = ceil(Int, cos_angle * (i - cx) - sin_angle * (j - cy) + cx)\n            j_rot = ceil(Int, sin_angle * (i - cx) + cos_angle * (j - cy) + cy)\n            if checkbounds(Bool, img, i_rot, j_rot) res[i, j] = bilinear_interpolation(img, i_rot, j_rot) end\n        end\n    end\n    res = shareproperties(img, res)\n    res\nend\n\n\n\n\nNow, let us create the two images we will match using BRIEF. \n\n\n```@example 1\n\n\nusing ImageFeatures, TestImages, Images, ImageDraw\n\n\nimg = testimage(\"lighthouse\")\nimg_array_1 = convert(Array{Gray}, img)\nimg_array_2 = _warp(img_array_1, pi / 4)\n\n\nnothing # hide\n\n\n\n\nThe ORB descriptor calculates the keypoints as well as the descriptor, unlike [BRIEF](brief). To create the ORB descriptor, we first need to define the parameters by calling the [`ORB`](../function_reference.md#ImageFeatures.ORB) constructor.\n\n\n```julia\norb_params = ORB(num_keypoints = 1000)\n\n\n\n\nNow pass the image with the parameters to the \ncreate_descriptor\n function.\n\n\n```@example 1\ndesc_1, ret_keypoints_1 = create_descriptor(img_array_1, orb_params)\ndesc_2, ret_keypoints_2 = create_descriptor(img_array_2, orb_params)\nnothing # hide\n\n\n\n\nThe obtained descriptors can be used to find the matches between the two images using the [`match_keypoints`](../function_reference.md#ImageFeatures.match_keypoints) function.\n\n\n```@example 1\nmatches = match_keypoints(ret_keypoints_1, ret_keypoints_2, desc_1, desc_2, 0.2)\nnothing # hide\n\n\n\n\nWe can use the \nImageDraw.jl\n package to view the results.\n\n\n```@example 1\n\n\ngrid = hcat(img_array_1, img_array_2)\noffset = CartesianIndex(0, 768)\nmap(m_i -\n line!(grid, m_i[1], m_i[2] + offset), matches)\nsave(\"orb_example.jpg\", grid); nothing # hide\n\n\n```", 
            "title": "ORB"
        }, 
        {
            "location": "/tutorials/orb/#example", 
            "text": "Let us take a look at a simple example where the ORB descriptor is used to match two images where one has been translated by  (50, 40)  pixels and then rotated by an angle of 75 degrees. We will use the  lighthouse  image from the  TestImages  package for this example.  First, lets define warping functions to transform and rotate the image.  function _warp(img, transx, transy)\n    res = zeros(eltype(img), size(img))\n    for i in 1:size(img, 1) - transx\n        for j in 1:size(img, 2) - transy\n            res[i + transx, j + transy] = img[i, j]\n        end\n    end\n    res = shareproperties(img, res)\n    res\nend\n\nfunction _warp(img, angle)\n    cos_angle = cos(angle)\n    sin_angle = sin(angle)\n    res = zeros(eltype(img), size(img))\n    cx = size(img, 1) / 2\n    cy = size(img, 2) / 2\n    for i in 1:size(res, 1)\n        for j in 1:size(res, 2)\n            i_rot = ceil(Int, cos_angle * (i - cx) - sin_angle * (j - cy) + cx)\n            j_rot = ceil(Int, sin_angle * (i - cx) + cos_angle * (j - cy) + cy)\n            if checkbounds(Bool, img, i_rot, j_rot) res[i, j] = bilinear_interpolation(img, i_rot, j_rot) end\n        end\n    end\n    res = shareproperties(img, res)\n    res\nend  Now, let us create the two images we will match using BRIEF.   ```@example 1  using ImageFeatures, TestImages, Images, ImageDraw  img = testimage(\"lighthouse\")\nimg_array_1 = convert(Array{Gray}, img)\nimg_array_2 = _warp(img_array_1, pi / 4)  nothing # hide  \n\nThe ORB descriptor calculates the keypoints as well as the descriptor, unlike [BRIEF](brief). To create the ORB descriptor, we first need to define the parameters by calling the [`ORB`](../function_reference.md#ImageFeatures.ORB) constructor.\n\n\n```julia\norb_params = ORB(num_keypoints = 1000)  Now pass the image with the parameters to the  create_descriptor  function.  ```@example 1\ndesc_1, ret_keypoints_1 = create_descriptor(img_array_1, orb_params)\ndesc_2, ret_keypoints_2 = create_descriptor(img_array_2, orb_params)\nnothing # hide  \n\nThe obtained descriptors can be used to find the matches between the two images using the [`match_keypoints`](../function_reference.md#ImageFeatures.match_keypoints) function.\n\n\n```@example 1\nmatches = match_keypoints(ret_keypoints_1, ret_keypoints_2, desc_1, desc_2, 0.2)\nnothing # hide  We can use the  ImageDraw.jl  package to view the results.  ```@example 1  grid = hcat(img_array_1, img_array_2)\noffset = CartesianIndex(0, 768)\nmap(m_i -  line!(grid, m_i[1], m_i[2] + offset), matches)\nsave(\"orb_example.jpg\", grid); nothing # hide  ```", 
            "title": "Example"
        }, 
        {
            "location": "/tutorials/brisk/", 
            "text": "The \nBRISK\n descriptor has a predefined sampling pattern as compared to \nBRIEF\n or \nORB\n. Pixels are sampled over concentric rings. For each sampling point, a small patch is considered around it. Before starting the algorithm, the patch is smoothed using gaussian smoothing.\n\n\n\n\nTwo types of pairs are used for sampling, short and long pairs. Short pairs are those where the distance is below a set threshold distmax while the long pairs have distance above distmin. Long pairs are used for orientation and short pairs are used for calculating the descriptor by comparing intensities.\n\n\nBRISK achieves rotation invariance by trying the measure orientation of the keypoint and rotating the sampling pattern by that orientation. This is done by first calculating the local gradient \ng(pi,pj)\n between sampling pair \n(pi,pj)\n where \nI(pj, pj)\n is the smoothed intensity after applying gaussian smoothing.\n\n\ng(pi, pj) = (pi - pj) . I(pj, j) -I(pj, j)pj - pi2\n\n\nAll local gradients between long pairs and then summed and the \narctangent(gy/gx)\n between \ny\n and \nx\n components of the sum is taken as the angle of the keypoint. Now, we only need to rotate the short pairs by that angle to help the descriptor become more invariant to rotation.  The descriptor is built using intensity comparisons. For each short pair if the first point has greater intensity than the second, then 1 is written else 0 is written to the corresponding bit of the descriptor.", 
            "title": "BRISK"
        }, 
        {
            "location": "/tutorials/freak/", 
            "text": "FREAK\n has a defined sampling pattern like \nBRISK\n. It uses a retinal sampling grid with more density of points near the centre  with the density decreasing exponentially with distance from the centre.\n\n\n\n\nFREAK\u2019s measure of orientation is similar to \nBRISK\n but instead of using long pairs, it uses a set of predefined 45 symmetric sampling pairs. The set of sampling pairs is determined using a method similar to \nORB\n, by finding sampling pairs over keypoints in standard datasets and then extracting the most discriminative pairs. The orientation weights over these pairs are summed and the sampling window is rotated by this orientation to some canonical orientation to achieve rotation invariance.\n\n\nThe descriptor is built using intensity comparisons of a predetermined set of 512 sampling pairs. This set is also obtained using a method similar to the one described above. For each pair if the first point has greater intensity than the second, then 1 is written else 0 is written to the corresponding bit of the descriptor.", 
            "title": "FREAK"
        }, 
        {
            "location": "/tutorials/glcm/", 
            "text": "Gray Level Co-occurrence Matrix (GLCM)\n is used for texture analysis. We consider two pixels at a time, called the reference and the neighbour pixel. We define a particular spatial relationship between the reference and neighbour pixel before calculating the GLCM. For eg, we may define the neighbour to be 1 pixel to the right of the current pixel, or it can be 3 pixels above, or 2 pixels diagonally (one of NE, NW, SE, SW) from the reference. \n\n\nOnce a spatial relationship is defined, we create a GLCM of size (Range of Intensities x Range of Intensities) all initialised to \n0\n. For eg, a \n8\n bit single channel Image will have a \n256x256\n GLCM. We then traverse through the image and for every pair of intensities we find for the defined spatial relationship, we increment that cell of the matrix. \n\n\n\n\nEach entry of the GLCM[i,j] holds the count of the number of times that pair of intensities appears in the image with the defined spatial relationship.\n\n\nThe matrix may be made symmetrical by adding it to its transpose and normalised to that each cell expresses the probability of that pair of intensities occurring in the image.\n\n\nOnce the GLCM is calculated, we can find texture properties from the matrix to represent the textures in the image.\n\n\n\n\nGLCM Properties\n\n\nThe properties can be calculated over the entire matrix or by considering a window which is moved along the matrix.\n\n\n\n\nMean\n\n\n\n\nVariance\n\n\n\n\nCorrelation\n\n\n\n\nContrast\n\n\n\n\nIDM (Inverse Difference Moment)\n\n\n\n\nASM (Angular Second Moment)\n\n\n\n\nEntropy\n\n\n\n\nMax Probability\n\n\n\n\nEnergy\n\n\n\n\nDissimilarity", 
            "title": "Gray Level Co-occurence Matrix"
        }, 
        {
            "location": "/tutorials/glcm/#glcm-properties", 
            "text": "The properties can be calculated over the entire matrix or by considering a window which is moved along the matrix.", 
            "title": "GLCM Properties"
        }, 
        {
            "location": "/tutorials/glcm/#mean", 
            "text": "", 
            "title": "Mean"
        }, 
        {
            "location": "/tutorials/glcm/#variance", 
            "text": "", 
            "title": "Variance"
        }, 
        {
            "location": "/tutorials/glcm/#correlation", 
            "text": "", 
            "title": "Correlation"
        }, 
        {
            "location": "/tutorials/glcm/#contrast", 
            "text": "", 
            "title": "Contrast"
        }, 
        {
            "location": "/tutorials/glcm/#idm-inverse-difference-moment", 
            "text": "", 
            "title": "IDM (Inverse Difference Moment)"
        }, 
        {
            "location": "/tutorials/glcm/#asm-angular-second-moment", 
            "text": "", 
            "title": "ASM (Angular Second Moment)"
        }, 
        {
            "location": "/tutorials/glcm/#entropy", 
            "text": "", 
            "title": "Entropy"
        }, 
        {
            "location": "/tutorials/glcm/#max-probability", 
            "text": "", 
            "title": "Max Probability"
        }, 
        {
            "location": "/tutorials/glcm/#energy", 
            "text": "", 
            "title": "Energy"
        }, 
        {
            "location": "/tutorials/glcm/#dissimilarity", 
            "text": "", 
            "title": "Dissimilarity"
        }, 
        {
            "location": "/tutorials/lbp/", 
            "text": "Local Binary Pattern (LBP)\n is a very efficient texture operator which labels the pixels of an image by thresholding the neighborhood of each pixel and considers the result as a binary number. The LBP feature vector, in its simplest form, is created in the following manner :\n\n\n\n\n\n\nDivide the examined window into cells (e.g. 16x16 pixels for each cell).\n\n\nFor each pixel in a cell, compare the pixel to each of its 8 neighbors (on its left-top, left-middle, left-bottom, right-top, etc.). Follow the pixels along a circle, i.e. clockwise or counterclockwise.\n\n\nIn the above step, the neighbours considered can be changed by varying the radius of the circle around the pixel, R and the quantisation of the angular space P.\n\n\nWhere the center pixel's value is greater than the neighbor's value, write \"0\". Otherwise, write \"1\". This gives an 8-digit binary number (which is usually converted to decimal for convenience).\n\n\nCompute the histogram, over the cell, of the frequency of each \"number\" occurring (i.e., each combination of which pixels are smaller and which are greater than the center). This histogram can be seen as a 256-dimensional feature vector.\n\n\nOptionally normalize the histogram.\n\n\nConcatenate (normalized) histograms of all cells. This gives a feature vector for the entire window.\n\n\n\n\nThe feature vector can now then be processed using some machine-learning algorithm to classify images. Such classifiers are often used for face recognition or texture analysis.\n\n\n\n\nTypes of Local Binary Patterns in ImageFeatures.jl\n\n\nImageFeatures.jl provides the following types of local binary patterns :\n\n\n\n\nlbp\n\n\nThe original local binary patterns \n\n\n\n\nmodified_lbp\n\n\n\n\ndirection_coded_lbp\n\n\n\n\nmulti_block_lbp", 
            "title": "Local Binary Patterns"
        }, 
        {
            "location": "/tutorials/lbp/#types-of-local-binary-patterns-in-imagefeaturesjl", 
            "text": "ImageFeatures.jl provides the following types of local binary patterns :", 
            "title": "Types of Local Binary Patterns in ImageFeatures.jl"
        }, 
        {
            "location": "/tutorials/lbp/#lbp", 
            "text": "The original local binary patterns", 
            "title": "lbp"
        }, 
        {
            "location": "/tutorials/lbp/#modified_lbp", 
            "text": "", 
            "title": "modified_lbp"
        }, 
        {
            "location": "/tutorials/lbp/#direction_coded_lbp", 
            "text": "", 
            "title": "direction_coded_lbp"
        }, 
        {
            "location": "/tutorials/lbp/#multi_block_lbp", 
            "text": "", 
            "title": "multi_block_lbp"
        }, 
        {
            "location": "/function_reference/", 
            "text": "Feature Extraction and Descriptors\n\n\nBelow \n[]\n in an argument list means an optional argument.\n\n\n\n\nTypes\n\n\n#\n\n\nImageFeatures.Keypoint\n \n \nType\n.\n\n\nkeypoint = Keypoint(y, x)\n\n\n\n\nThe \nKeypoint\n type.\n\n\nsource\n\n\n#\n\n\nImageFeatures.Keypoints\n \n \nType\n.\n\n\nkeypoints = Keypoints(boolean_img)\n\n\n\n\nCreates a \nVector{Keypoint}\n of the true values in the \nboolean_img\n.\n\n\nsource\n\n\n#\n\n\nImageFeatures.BRIEF\n \n \nType\n.\n\n\nbrief_params = BRIEF([size = 128], [window = 9], [sigma = 2 ^ 0.5], [sampling_type = gaussian], [seed = 123])\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsize\n\n\nInt\n\n\nSize of the descriptor\n\n\n\n\n\n\nwindow\n\n\nInt\n\n\nSize of sampling window\n\n\n\n\n\n\nsigma\n\n\nFloat64\n\n\nValue of sigma used for inital gaussian smoothing of image\n\n\n\n\n\n\nsampling_type\n\n\nFunction\n\n\nType of sampling used for building the descriptor (See \nBRIEF Sampling Patterns\n)\n\n\n\n\n\n\nseed\n\n\nInt\n\n\nRandom seed used for generating the sampling pairs. For matching two descriptors, the seed used to build both should be same.\n\n\n\n\n\n\n\n\nsource\n\n\n#\n\n\nImageFeatures.ORB\n \n \nType\n.\n\n\norb_params = ORB([num_keypoints = 500], [n_fast = 12], [threshold = 0.25], [harris_factor = 0.04], [downsample = 1.3], [levels = 8], [sigma = 1.2])\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nnum_keypoints\n\n\nInt\n\n\nNumber of keypoints to extract and size of the descriptor calculated\n\n\n\n\n\n\nn_fast\n\n\nInt\n\n\nNumber of consecutive pixels used for finding corners with FAST. See [\nfastcorners\n]\n\n\n\n\n\n\nthreshold\n\n\nFloat64\n\n\nThreshold used to find corners in FAST. See [\nfastcorners\n]\n\n\n\n\n\n\nharris_factor\n\n\nFloat64\n\n\nHarris factor \nk\n used to rank keypoints by harris responses and extract the best ones\n\n\n\n\n\n\ndownsample\n\n\nFloat64\n\n\nDownsampling parameter used while building the gaussian pyramid. See [\ngaussian_pyramid\n] in Images.jl\n\n\n\n\n\n\nlevels\n\n\nInt\n\n\nNumber of levels in the gaussian pyramid.  See [\ngaussian_pyramid\n] in Images.jl\n\n\n\n\n\n\nsigma\n\n\nFloat64\n\n\nUsed for gaussian smoothing in each level of the gaussian pyramid.  See [\ngaussian_pyramid\n] in Images.jl\n\n\n\n\n\n\n\n\nsource\n\n\n#\n\n\nImageFeatures.FREAK\n \n \nType\n.\n\n\nfreak_params = FREAK([pattern_scale = 22.0])\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npattern_scale\n\n\nFloat64\n\n\nScaling factor for the sampling window\n\n\n\n\n\n\n\n\nsource\n\n\n\n\nCorners\n\n\n#\n\n\nImageFeatures.corner_orientations\n \n \nFunction\n.\n\n\norientations = corner_orientations(img)\norientations = corner_orientations(img, corners)\norientations = corner_orientations(img, corners, kernel)\n\n\n\n\nReturns the orientations of corner patches in an image. The orientation of a corner patch is denoted by the orientation of the vector between intensity centroid and the corner. The intensity centroid can be calculated as \nC = (m01/m00, m10/m00)\n where mpq is defined as -\n\n\n`mpq = (x^p)(y^q)I(y, x) for each p, q in the corner patch`\n\n\n\n\nThe kernel used for the patch can be given through the \nkernel\n argument. The default kernel used is a gaussian kernel of size \n5x5\n.\n\n\nsource\n\n\n\n\nBRIEF Sampling Patterns\n\n\n#\n\n\nImageFeatures.random_uniform\n \n \nFunction\n.\n\n\nsample_one, sample_two = random_uniform(size, window, seed)\n\n\n\n\nBuilds sampling pairs using random uniform sampling.\n\n\nsource\n\n\n#\n\n\nImageFeatures.random_coarse\n \n \nFunction\n.\n\n\nsample_one, sample_two = random_coarse(size, window, seed)\n\n\n\n\nBuilds sampling pairs using random sampling over a coarse grid.\n\n\nsource\n\n\n#\n\n\nImageFeatures.gaussian\n \n \nFunction\n.\n\n\nsample_one, sample_two = gaussian(size, window, seed)\n\n\n\n\nBuilds sampling pairs using gaussian sampling.\n\n\nsource\n\n\n#\n\n\nImageFeatures.gaussian_local\n \n \nFunction\n.\n\n\nsample_one, sample_two = gaussian_local(size, window, seed)\n\n\n\n\nPairs \n(Xi, Yi)\n are randomly sampled using a Gaussian distribution where first \nX\n is sampled with a standard deviation of \n0.04*S^2\n and  then the \nYi\u2019s\n are sampled using a Gaussian distribution \u2013 Each \nYi\n is sampled with mean \nXi\n and standard deviation of \n0.01 * S^2\n\n\nsource\n\n\n#\n\n\nImageFeatures.centered\n \n \nFunction\n.\n\n\nsample_one, sample_two = centered(size, window, seed)\n\n\n\n\nBuilds sampling pairs \n(Xi, Yi)\n where \nXi\n is \n(0, 0)\n and \nYi\n is sampled uniformly from the window.\n\n\nsource\n\n\n\n\nFeature Extraction\n\n\n\n\nFeature Description\n\n\ncreate_descriptor\n\n\n\n\n\n\nFeature Matching\n\n\n#\n\n\nImageFeatures.hamming_distance\n \n \nFunction\n.\n\n\ndistance = hamming_distance(desc_1, desc_2)\n\n\n\n\nCalculates the hamming distance between two descriptors.\n\n\nsource\n\n\n#\n\n\nImageFeatures.match_keypoints\n \n \nFunction\n.\n\n\nmatches = match_keypoints(keypoints_1, keypoints_2, desc_1, desc_2, threshold = 0.1)\n\n\n\n\nFinds matched keypoints using the \nhamming_distance\n function having distance value less than \nthreshold\n.\n\n\nsource\n\n\n\n\nTexture Matching\n\n\n\n\nGray Level Co-occurence Matrix\n\n\nglcm\nglcm_symmetric\nglcm_norm\nglcm_prop\nmax_prob\ncontrast\nASM\nIDM\nglcm_entropy\nenergy\ncontrast\ndissimilarity\ncorrelation\nglcm_mean_ref\nglcm_mean_neighbour\nglcm_var_ref\nglcm_var_neighbour\n\n\n\n\n\n\nLocal Binary Patterns\n\n\nlbp\nmodified_lbp\ndirection_coded_lbp\nlbp_original\nlbp_uniform\nlbp_rotation_invariant\nmulti_block_lbp", 
            "title": "Function Reference"
        }, 
        {
            "location": "/function_reference/#feature-extraction-and-descriptors", 
            "text": "Below  []  in an argument list means an optional argument.", 
            "title": "Feature Extraction and Descriptors"
        }, 
        {
            "location": "/function_reference/#types", 
            "text": "#  ImageFeatures.Keypoint     Type .  keypoint = Keypoint(y, x)  The  Keypoint  type.  source  #  ImageFeatures.Keypoints     Type .  keypoints = Keypoints(boolean_img)  Creates a  Vector{Keypoint}  of the true values in the  boolean_img .  source  #  ImageFeatures.BRIEF     Type .  brief_params = BRIEF([size = 128], [window = 9], [sigma = 2 ^ 0.5], [sampling_type = gaussian], [seed = 123])     Argument  Type  Description      size  Int  Size of the descriptor    window  Int  Size of sampling window    sigma  Float64  Value of sigma used for inital gaussian smoothing of image    sampling_type  Function  Type of sampling used for building the descriptor (See  BRIEF Sampling Patterns )    seed  Int  Random seed used for generating the sampling pairs. For matching two descriptors, the seed used to build both should be same.     source  #  ImageFeatures.ORB     Type .  orb_params = ORB([num_keypoints = 500], [n_fast = 12], [threshold = 0.25], [harris_factor = 0.04], [downsample = 1.3], [levels = 8], [sigma = 1.2])     Argument  Type  Description      num_keypoints  Int  Number of keypoints to extract and size of the descriptor calculated    n_fast  Int  Number of consecutive pixels used for finding corners with FAST. See [ fastcorners ]    threshold  Float64  Threshold used to find corners in FAST. See [ fastcorners ]    harris_factor  Float64  Harris factor  k  used to rank keypoints by harris responses and extract the best ones    downsample  Float64  Downsampling parameter used while building the gaussian pyramid. See [ gaussian_pyramid ] in Images.jl    levels  Int  Number of levels in the gaussian pyramid.  See [ gaussian_pyramid ] in Images.jl    sigma  Float64  Used for gaussian smoothing in each level of the gaussian pyramid.  See [ gaussian_pyramid ] in Images.jl     source  #  ImageFeatures.FREAK     Type .  freak_params = FREAK([pattern_scale = 22.0])     Argument  Type  Description      pattern_scale  Float64  Scaling factor for the sampling window     source", 
            "title": "Types"
        }, 
        {
            "location": "/function_reference/#corners", 
            "text": "#  ImageFeatures.corner_orientations     Function .  orientations = corner_orientations(img)\norientations = corner_orientations(img, corners)\norientations = corner_orientations(img, corners, kernel)  Returns the orientations of corner patches in an image. The orientation of a corner patch is denoted by the orientation of the vector between intensity centroid and the corner. The intensity centroid can be calculated as  C = (m01/m00, m10/m00)  where mpq is defined as -  `mpq = (x^p)(y^q)I(y, x) for each p, q in the corner patch`  The kernel used for the patch can be given through the  kernel  argument. The default kernel used is a gaussian kernel of size  5x5 .  source", 
            "title": "Corners"
        }, 
        {
            "location": "/function_reference/#brief-sampling-patterns", 
            "text": "#  ImageFeatures.random_uniform     Function .  sample_one, sample_two = random_uniform(size, window, seed)  Builds sampling pairs using random uniform sampling.  source  #  ImageFeatures.random_coarse     Function .  sample_one, sample_two = random_coarse(size, window, seed)  Builds sampling pairs using random sampling over a coarse grid.  source  #  ImageFeatures.gaussian     Function .  sample_one, sample_two = gaussian(size, window, seed)  Builds sampling pairs using gaussian sampling.  source  #  ImageFeatures.gaussian_local     Function .  sample_one, sample_two = gaussian_local(size, window, seed)  Pairs  (Xi, Yi)  are randomly sampled using a Gaussian distribution where first  X  is sampled with a standard deviation of  0.04*S^2  and  then the  Yi\u2019s  are sampled using a Gaussian distribution \u2013 Each  Yi  is sampled with mean  Xi  and standard deviation of  0.01 * S^2  source  #  ImageFeatures.centered     Function .  sample_one, sample_two = centered(size, window, seed)  Builds sampling pairs  (Xi, Yi)  where  Xi  is  (0, 0)  and  Yi  is sampled uniformly from the window.  source", 
            "title": "BRIEF Sampling Patterns"
        }, 
        {
            "location": "/function_reference/#feature-extraction", 
            "text": "", 
            "title": "Feature Extraction"
        }, 
        {
            "location": "/function_reference/#feature-description", 
            "text": "create_descriptor", 
            "title": "Feature Description"
        }, 
        {
            "location": "/function_reference/#feature-matching", 
            "text": "#  ImageFeatures.hamming_distance     Function .  distance = hamming_distance(desc_1, desc_2)  Calculates the hamming distance between two descriptors.  source  #  ImageFeatures.match_keypoints     Function .  matches = match_keypoints(keypoints_1, keypoints_2, desc_1, desc_2, threshold = 0.1)  Finds matched keypoints using the  hamming_distance  function having distance value less than  threshold .  source", 
            "title": "Feature Matching"
        }, 
        {
            "location": "/function_reference/#texture-matching", 
            "text": "", 
            "title": "Texture Matching"
        }, 
        {
            "location": "/function_reference/#gray-level-co-occurence-matrix", 
            "text": "glcm\nglcm_symmetric\nglcm_norm\nglcm_prop\nmax_prob\ncontrast\nASM\nIDM\nglcm_entropy\nenergy\ncontrast\ndissimilarity\ncorrelation\nglcm_mean_ref\nglcm_mean_neighbour\nglcm_var_ref\nglcm_var_neighbour", 
            "title": "Gray Level Co-occurence Matrix"
        }, 
        {
            "location": "/function_reference/#local-binary-patterns", 
            "text": "lbp\nmodified_lbp\ndirection_coded_lbp\nlbp_original\nlbp_uniform\nlbp_rotation_invariant\nmulti_block_lbp", 
            "title": "Local Binary Patterns"
        }, 
        {
            "location": "/CONTRIBUTING/", 
            "text": "", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/release_notes/", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/LICENSE/", 
            "text": "The ImageFeatures.jl package is licensed under the MIT \"Expat\" License:\n\n\n\n\nCopyright (c) 2016: mronian.\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", 
            "title": "License"
        }
    ]
}